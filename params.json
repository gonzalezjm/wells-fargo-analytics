{"name":"Wells Fargo Analytics Competition","tagline":"Fall 2015 Data Science","body":"\r\n##Process Flow Diagram\r\n![Process-Diagram-img](http://i.imgur.com/UBQ4LBQ.png)\r\n\r\n##Description\r\n\r\nIn order to attain a workable dataset, we import dataset.txt into a data frame, then clean the FullText column of non-ASCII characters, punctuation, and other “trouble” terms we might come across, such as words with no meaningful value in the context of the analysis. We then add a column to our data frame called “BankID”, which allows us to keep track of which bank each comment references.\r\nThe code to complete this is:\r\n\r\n```R\r\n# loads the text file with the data and puts it into a data frame\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\n\r\n# FullText: the column in the dataframe that contains the tweets/statuses\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Grab just the texts, to load them into the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\n# stores the new cleaned FullText into the data frame in place of the original FullText column\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n# If you want to test on just 10000 records using df.10000 created below (can be any # of records)\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))  \r\n \r\ndocs <- tm_map(docs, PlainTextDocument)  \r\n\r\n# remove punctuation\r\ndocs <- tm_map(docs, removePunctuation) \r\n\r\n# Strip extra whitespace\r\ndocs <- tm_map(docs, stripWhitespace)\r\n\r\n#Grab indexes of comments for different banks\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\n#give each bank a name\r\ndf$BankID = NaN*df$MediaType # Only do this because I want it to be the right size\r\ndf$BankID[bankA.idx] = \"BankA\"\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\n#put data into docs for each bank\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx] \r\n\r\n```\r\n\r\nOnce we have the data properly loaded and sorted, we can start creating different plots/visuals. In our case, we ran word frequencies, word associations, and word cluster dendrograms. As we looked at different plots, I started removing terms that I thought were not useful from the data. Some terms were very stubborn and could not be removed from the data for some reason. This was a recurring problem throughout the project. Below is the exact list of words I decided to remove from the data.\r\n\r\n```R\r\ndocs <- tm_map(docs, removeWords,c(\"their\",\"from\", \"had\", \"who\", \"was\", \"have\", \"you\", \"but\", \"they\", \"its\", \"just\", \"got\", \"one\", \"will\", \"with\", \"has\",\"Name\", \"name\", \"nameresp\", \"dirmsg\", \"and\",\"for\",\"name\", \"twithndl\", \"let\",\"this\", \"are\", \"what\", \"would\", \"here\", \"nameresp\", \"that\", \"the\", \"rettwit\", \"https\", \"http\", \"twithndlbanka\")) \r\n```\r\n\r\nOnce we had a set of words that we were content with, we found the frequent terms, created a word cloud and created a cluster dendrogram for the data as a whole. The code snippets below show how we were able to achieve that.\r\n\r\n#Finding frequent terms:\r\n```R\r\ntdm <- TermDocumentMatrix(docs)\r\ndtm <- DocumentTermMatrix(docs)\r\n\r\nfindFreqTerms(dtm,2000)\r\n\r\nfreq <- colSums(as.matrix(dtm))  \r\nord <- order(freq)   \r\nfreq[head(ord)]  \r\nfreq[tail(ord)]\r\nfreq[ord[(length(ord)-100):length(ord)]]\r\n\r\ndtm = removeSparseTerms(dtm, 0.98)\r\n```\r\n\r\n#Creating a word cloud:\r\n```R\r\nlibrary(wordcloud)   \r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")  \r\nwordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2) # 100 most frequent words\r\n```\r\n\r\n###Word Cloud\r\n![Word-Cloud-img](http://i.imgur.com/29NPvyZ.png)\r\n\r\n\r\n#Creating a cluster dendrogram:\r\n```R\r\ndtmss <- removeSparseTerms(dtm, 0.96) \r\n#inspect(dtmss)\r\n\r\nlibrary(cluster)   \r\nd <- dist(t(dtmss), method=\"euclidian\")   \r\nfit <- hclust(d=d, method=\"ward.D\")   \r\n\r\nplot(fit, hang=-1)  \r\n\r\ngroups <- cutree(fit, k=5)   # \"k=\" defines the number of clusters you are using   \r\nrect.hclust(fit, k=5, border=\"red\") # draw dendogram with red borders around the 5 clusters  \r\n```\r\n\r\n###Cluster Dendrogram\r\n![Cluster-Dendrogram-img](http://i.imgur.com/U3k8IkR.png)\r\n\r\n\r\n\r\n```R\r\nfindAssocs(dtmss, c(\"banka\", \"bankb\", \"bankc\", \"bankd\"), corlimit=0.01)\r\n\r\nfindAssocs(dtmss, c(\"credit\",\"internet\", \"business\",\"account\",\"financial\", \"over\", \"money\", \"phone\"), corlimit=0.01)\r\n\r\nfindAssocs(dtmss, c(\"rating\", \"thank\", \"card\", \"time\"), corlimit=0.01)\r\n```\r\nOnce we have our data properly sorted, we create five Document-Term Matrices and five Term-Document Matrices; one of each for the entire dataset, and one of each for each bank. Using the Document-Term matrices, we can analyze the frequencies at which each term appears in the dataset, in the entire dataset or within the context of a specific bank. This allows us to see the most commonly used words, and consequently, the most commonly referenced topics.\r\n\r\nEach comment is scored on its “sentiment” by matching the terms in the comment to two libraries of “positive” and “negative” terms. Each score is found by subtracting the number of negative terms in the comment from the number of positive terms in the comment. We then use “very positive” (score > 1) and “very negative” (score < -1) scores to calculate the global score using the formula GlobalScore = 100 * verypos / (verypos + veryneg).\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}