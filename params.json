{"name":"Wells Fargo Analytics Competition","tagline":"Fall 2015 Data Science","body":"This wiki page is meant to show how, why and the results of our analysis of the Wells Fargo dataset from the Wells Fargo MindSumo challenge. To start, here is a process flow diagram.\r\n\r\n#Process Flow Diagram\r\n![Process-Diagram-img](http://i.imgur.com/UBQ4LBQ.png)\r\n\r\n#Description\r\n\r\nIn order to attain a workable dataset, we import dataset.txt into a data frame, then clean the FullText column of non-ASCII characters, punctuation, and other “trouble” terms we might come across, such as words with no meaningful value in the context of the analysis. We then add a column to our data frame called “BankID”, which allows us to keep track of which bank each comment references.\r\nThe code to complete this is:\r\n\r\n```R\r\n# loads the text file with the data and puts it into a data frame\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\n\r\n# FullText: the column in the dataframe that contains the tweets/statuses\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Grab just the texts, to load them into the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\n# stores the new cleaned FullText into the data frame in place of the original FullText column\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n# If you want to test on just 10000 records using df.10000 created below (can be any # of records)\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))  \r\n \r\ndocs <- tm_map(docs, PlainTextDocument)  \r\n\r\n# remove punctuation\r\ndocs <- tm_map(docs, removePunctuation) \r\n\r\n# Strip extra whitespace\r\ndocs <- tm_map(docs, stripWhitespace)\r\n\r\n#Grab indexes of comments for different banks\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\n#give each bank a name\r\ndf$BankID = NaN*df$MediaType # Only do this because I want it to be the right size\r\ndf$BankID[bankA.idx] = \"BankA\"\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\n#put data into docs for each bank\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx] \r\n\r\n```\r\n\r\nOnce we have the data properly loaded and sorted, we can start creating different plots/visuals. In our case, we ran word frequencies, word associations, and word cluster dendrograms. As we looked at different plots, I started removing terms that I thought were not useful from the data. Some terms were very stubborn and could not be removed from the data for some reason. This was a recurring problem throughout the project. Below is the exact list of words I decided to remove from the data.\r\n\r\n```R\r\ndocs <- tm_map(docs, removeWords,c(\"their\",\"from\", \"had\", \"who\", \"was\", \"have\", \"you\", \"but\", \"they\", \"its\", \"just\", \"got\", \"one\", \"will\", \"with\", \"has\",\"Name\", \"name\", \"nameresp\", \"dirmsg\", \"and\",\"for\",\"name\", \"twithndl\", \"let\",\"this\", \"are\", \"what\", \"would\", \"here\", \"nameresp\", \"that\", \"the\", \"rettwit\", \"https\", \"http\", \"twithndlbanka\")) \r\n```\r\n\r\n#Frequent Terms, Word Clouds and Cluster Dendrograms\r\n\r\nOnce we had a set of words that we were content with, we found the frequent terms, created a word cloud and created a cluster dendrogram for the data as a whole. The code snippets below show how we were able to achieve that.\r\n\r\n##Finding frequent terms:\r\n```R\r\ntdm <- TermDocumentMatrix(docs)\r\ndtm <- DocumentTermMatrix(docs)\r\n\r\nfindFreqTerms(dtm,2000)\r\n\r\nfreq <- colSums(as.matrix(dtm))  \r\nord <- order(freq)   \r\nfreq[head(ord)]  \r\nfreq[tail(ord)]\r\nfreq[ord[(length(ord)-100):length(ord)]]\r\n\r\ndtm = removeSparseTerms(dtm, 0.98)\r\n```\r\n\r\n##Creating a word cloud:\r\nA word cloud shows the most frequent words in the data. In our case, we chose to show the top 100 words.\r\n```R\r\nlibrary(wordcloud)   \r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")  \r\nwordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2) # 100 most frequent words\r\n```\r\n\r\n###Word Cloud Output\r\n![Word-Cloud-img](http://i.imgur.com/29NPvyZ.png)\r\n\r\nThe word cloud above shows the top 100 most frequent words in the data set. An example of a hypothesis that could be formed from this word cloud is that bankA and bankB are the most prevalent banks because they are mentioned the most.\r\n\r\n\r\n##Creating a cluster dendrogram:\r\nA cluster dendrogram uses euclidian distance to show relationships and hierarchies among the words. These can be very valuable if the data set is cleaned up in the right way.\r\n\r\n```R\r\ndtmss <- removeSparseTerms(dtm, 0.96) \r\n#inspect(dtmss)\r\n\r\nlibrary(cluster)   \r\nd <- dist(t(dtmss), method=\"euclidian\")   \r\nfit <- hclust(d=d, method=\"ward.D\")   \r\n\r\nplot(fit, hang=-1)  \r\n\r\ngroups <- cutree(fit, k=5)   # \"k=\" defines the number of clusters you are using   \r\nrect.hclust(fit, k=5, border=\"red\") # draw dendogram with red borders around the 5 clusters  \r\n```\r\n\r\n###Cluster Dendrogram Output\r\n![Cluster-Dendrogram-img](http://i.imgur.com/U3k8IkR.png)\r\n\r\nThe cluster dendrogram above doesn't show much, but if it was full scale, you would be able to see the hierarchy of related words. When we ran this with the full data set, it was not legible. But, with the right display form, it could be very useful.\r\n\r\n##Word Associations\r\n\r\nThe code below shows how to find the words associated with a list of terms. For example, the first line of the code would find words that are most associated with banka, bankb, bankc and bankd.\r\n\r\n```R\r\nfindAssocs(dtmss, c(\"banka\", \"bankb\", \"bankc\", \"bankd\"), corlimit=0.01)\r\n\r\nfindAssocs(dtmss, c(\"credit\",\"internet\", \"business\",\"account\",\"financial\", \"over\", \"money\", \"phone\"), corlimit=0.01)\r\n\r\nfindAssocs(dtmss, c(\"rating\", \"thank\", \"card\", \"time\"), corlimit=0.01)\r\n```\r\n\r\n###Word Association Output\r\n![Word-Association-img](http://i.imgur.com/tzm6zgt.png)\r\n\r\nThe image above shows the output for the first line of code (a word association for banka, bankb, bankc and bankd). From these associations, you can determine what some of the recurring tweet/status topics may be. For example, \"card\", \"account\", and \"credit\" could be recurring topics for some of the banks. Some of the associations that come up are not that useful, but that is up to the analyst to decide what is useful or not. For the sake of the project, it was more important to me to show why and how I did things, rather than to get a perfect result.\r\n\r\n#Analyzing Each Bank\r\n\r\nThe next step for us was analyzing each bank one by one. To do this, we completed the same steps that we did for the data set as a whole. But, we also added a bar graph for the most frequent terms.\r\n\r\n##Clean up data for One Bank\r\n```R\r\n#remove words from doc and create a term document matrix and document term matrix\r\nbankA.docs <- tm_map(bankA.docs, removeWords,c(\"twit_hndl_banka\",\"twit_hndl\", \"their\",\"from\", \"had\", \"who\", \"was\", \"have\", \"you\", \"but\", \"they\", \"its\", \"just\", \"got\", \"one\", \"will\", \"with\", \"has\",\"Name\", \"name\", \"nameresp\", \"dirmsg\", \"and\",\"for\",\"name\", \"twithndl\", \"let\", \"it.\", \"...\",\"name_resp\", \"them\", \"then\", \"tt/\",\"ly/\",\"bit.\",\"our\",\"these\", \"way\", \"this\", \"are\", \"what\", \"would\", \"here\", \"nameresp\", \"that\", \"the\", \"rettwit\", \"https\", \"http\", \"twithndlbanka\")) \r\n\r\ntdmA <- TermDocumentMatrix(bankA.docs)\r\ndtmA <- DocumentTermMatrix(bankA.docs)\r\n```\r\n\r\n##Frequent Terms for One Bank\r\n```R\r\n#find the frequent terms in the dataset for the respective bank\r\nfindFreqTerms(dtmA,2000)\r\n\r\nfreqA <- colSums(as.matrix(dtmA))  \r\nordA <- order(freqA)   \r\nfreq[head(ordA)]  \r\nfreq[tail(ordA)]\r\nfreq[ordA[(length(ordA)-100):length(ordA)]]\r\n\r\n#remove least used terms\r\ndtmA = removeSparseTerms(dtmA, 0.99)\r\n```\r\n\r\n##Word Cloud for One Bank\r\n```R\r\n#create wordcloud\r\nlibrary(wordcloud)   \r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")  \r\nwordcloud(names(freqA), freqA, max.words=100, rot.per=0.2, colors=dark2) # 100 most frequent words\r\n\r\ndtmssA <- removeSparseTerms(dtmA, 0.99) \r\n#inspect(dtmssA)\r\n```\r\n\r\n###Word Cloud for BankA\r\n![Word-CloudA-img](http://i.imgur.com/I1BwraB.png)\r\n\r\nAbove is the Word Cloud for Bank A.\r\n\r\n##Word Associations for One Bank\r\n```R\r\n#find word associations\r\nfindAssocs(dtmssA, c(\"banka\"), corlimit=0.02)\r\nfindAssocs(dtmA, c(\"banka\"), corlimit=0.02)\r\nfindAssocs(dtmA, c(\"banka\"), corlimit=0.02)\r\n```\r\n\r\n###Word Associations for Bank A\r\n![Word-AssociationsA-img](http://i.imgur.com/5BLy3oM.png)\r\n\r\n```R\r\n#create bar graph for the most frequently used terms\r\nfreq.termsA <- findFreqTerms(tdmA, lowfreq = 50)\r\ntermA.freq <- rowSums(as.matrix(tdmA))\r\n\r\ntermA.freq <- subset(termA.freq, termA.freq >= 50)\r\n\r\ndf.A <- data.frame(term = names(termA.freq), freq = termA.freq)\r\n\r\nlibrary(ggplot2)\r\nggplot(df.A, aes(x = term, y = freq)) + geom_bar(stat = \"identity\") + xlab(\"Terms\") + ylab(\"Count\") + coord_flip()\r\n```\r\n###Bar Graph for Frequent Terms for Bank A\r\n![Bar-GraphA-img](http://i.imgur.com/ra9gyfE.png)\r\n\r\nThe bar graph above shows the terms that are used 50 times or more in the data set for Bank A.\r\n\r\nOnce we have our data properly sorted, we create five Document-Term Matrices and five Term-Document Matrices; one of each for the entire dataset, and one of each for each bank. Using the Document-Term matrices, we can analyze the frequencies at which each term appears in the dataset, in the entire dataset or within the context of a specific bank. This allows us to see the most commonly used words, and consequently, the most commonly referenced topics.\r\n\r\nEach comment is scored on its “sentiment” by matching the terms in the comment to two libraries of “positive” and “negative” terms. Each score is found by subtracting the number of negative terms in the comment from the number of positive terms in the comment. We then use “very positive” (score > 1) and “very negative” (score < -1) scores to calculate the global score using the formula GlobalScore = 100 * verypos / (verypos + veryneg).\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}